{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"d7Ti1UYvMSw7"},"source":["# 載入函式庫 & 掛載雲端硬碟"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4420,"status":"ok","timestamp":1684398218900,"user":{"displayName":"Han-Tang Chen","userId":"09799993280170327597"},"user_tz":-480},"id":"ZtZokLx1Md34","outputId":"1d0bb3eb-71c9-4ad1-e8a9-0f1a75ce7cbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","from google.colab import drive\n","\n","import copy\n","import numpy\n","import numpy as np\n","import pandas as pd\n","import librosa\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Activation, BatchNormalization, Dense, LayerNormalization\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras import datasets, layers, models\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn import svm\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","import scipy.io.wavfile\n","from scipy.fftpack import dct\n","\n","drive.mount('/content/drive')\n","\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/Voice-Diease/')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"B-gRfMol-Bt8"},"source":["## 載入訓練資料"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684398218900,"user":{"displayName":"Han-Tang Chen","userId":"09799993280170327597"},"user_tz":-480},"id":"-g92u852-Bt8","outputId":"a8f1a55d-4136-4057-9a3b-457f673b28a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["source_df.shape : (1000, 28)\n","source_df.columns : Index(['ID', 'Sex', 'Age', 'Disease category', 'Narrow pitch range',\n","       'Decreased volume', 'Fatigue', 'Dryness', 'Lumping', 'heartburn',\n","       'Choking', 'Eye dryness', 'PND', 'Smoking', 'PPD', 'Drinking',\n","       'frequency', 'Diurnal pattern', 'Onset of dysphonia ', 'Noise at work',\n","       'Occupational vocal demand', 'Diabetes', 'Hypertension', 'CAD',\n","       'Head and Neck Cancer', 'Head injury', 'CVA',\n","       'Voice handicap index - 10'],\n","      dtype='object')\n"]}],"source":["# 讀取訓練資料集表單\n","source_df = pd.read_csv('./data/training_datalist.csv')\n","\n","print(\"source_df.shape :\", source_df.shape)\n","print(\"source_df.columns :\", source_df.columns)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Pv98B3-gq3x1"},"source":["## 切分訓練與驗證資料"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVy511D8Ezd7"},"outputs":[],"source":["class_weight = {0: 1.,\n","          1: 2.436,\n","          2: 3.19,\n","          3: 12.182,\n","          4: 16.75,}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684398218901,"user":{"displayName":"Han-Tang Chen","userId":"09799993280170327597"},"user_tz":-480},"id":"DhZBwCUNq3x1","outputId":"14316e20-045e-41ca-e1a3-31b221b7107f"},"outputs":[{"name":"stdout","output_type":"stream","text":["training_df shape : (850, 28) , test_df shape : (150, 28)\n"]}],"source":["training_df, validate_df = train_test_split(source_df, test_size=0.15, random_state=333)\n","\n","print(\"training_df shape :\", training_df.shape, \", test_df shape :\", validate_df.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pTOza8ZApwUU"},"source":["# 資料處理"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"i9jLL-4YpIVS"},"source":["## 文字資料前處理"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meyTRGyx6vAJ"},"outputs":[],"source":["def medical_data_proccessing(input_df):\n","    df = copy.deepcopy(input_df)\n","    # 將性別編碼0,1\n","    df['Sex'] = df['Sex'] - 1\n","\n","    # 將空值填0\n","    df['PPD'] = df['PPD'].fillna(0)\n","    df['Voice handicap index - 10'] = df['Voice handicap index - 10'].fillna(0)\n","\n","    # 正規化過大的數值\n","    #problem : use training data to normalize \n","    df['Age'] = df['Age'] / 100\n","    df['Voice handicap index - 10'] = df['Voice handicap index - 10'] / 40\n","    df['Occupational vocal demand'] /= 4\n","    df['Diurnal pattern'] /= 4\n","    df['Noise at work'] /= 3\n","    df['Onset of dysphonia '] /= 5\n","    df['frequency'] /= 3\n","    df['Drinking'] /= 2\n","    df['Smoking'] /= 3\n","    ## PPD just temp divided by 3\n","    df['PPD'] /= 3\n","\n","    return df"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Jv9Emm92o5zj"},"source":["## 音訊資料前處理"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLLH9LGjBXNf"},"outputs":[],"source":["def MFCCs(filename, second = 1):\n","  # setup\n","  sample_rate, raw_signal = scipy.io.wavfile.read(filename) # File assumed to be in the same directory\n","  signal = np.zeros(int(second*44100)) #fix audio to 3.5 seconds\n","  if(len(raw_signal) <= len(signal)):\n","    signal[:len(raw_signal)] = raw_signal\n","  else:\n","    signal = raw_signal[:len(signal)]\n","\n","  emphasized_signal = signal\n","  # #pre-emphasis\n","  # pre_emphasis = 0.97\n","  # emphasized_signal = numpy.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n","\n","  #framing\n","  frame_size = 0.025\n","  frame_stride = 0.01\n","\n","  frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples\n","  signal_length = len(emphasized_signal)\n","  frame_length = int(round(frame_length))\n","  frame_step = int(round(frame_step))\n","  num_frames = int(numpy.ceil(float(numpy.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n","\n","  pad_signal_length = num_frames * frame_step + frame_length\n","  z = numpy.zeros((pad_signal_length - signal_length))\n","  pad_signal = numpy.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n","\n","  indices = numpy.tile(numpy.arange(0, frame_length), (num_frames, 1)) + numpy.tile(numpy.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n","  frames = pad_signal[indices.astype(numpy.int32, copy=False)]\n","\n","  #window\n","  frames *= numpy.hamming(frame_length)\n","  # frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (frame_length - 1))  # Explicit Implementation **\n","\n","  #Fourier-Transform and Power Spectrum\n","  NFFT = 512\n","  mag_frames = numpy.absolute(numpy.fft.rfft(frames, NFFT))  # Magnitude of the FFT\n","  pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum\n","\n","  #Filter Banks\n","  nfilt =40\n","  low_freq_mel = 0\n","  high_freq_mel = (2595 * numpy.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\n","  mel_points = numpy.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n","  hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n","  bin = numpy.floor((NFFT + 1) * hz_points / sample_rate)\n","\n","  fbank = numpy.zeros((nfilt, int(numpy.floor(NFFT / 2 + 1))))\n","  for m in range(1, nfilt + 1):\n","      f_m_minus = int(bin[m - 1])   # left\n","      f_m = int(bin[m])             # center\n","      f_m_plus = int(bin[m + 1])    # right\n","\n","      for k in range(f_m_minus, f_m):\n","          fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n","      for k in range(f_m, f_m_plus):\n","          fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n","  filter_banks = numpy.dot(pow_frames, fbank.T)\n","  filter_banks = numpy.where(filter_banks == 0, numpy.finfo(float).eps, filter_banks)  # Numerical Stability\n","  filter_banks = 20 * numpy.log10(filter_banks)  # dB\n","  filter_banks -= (numpy.mean(filter_banks, axis=0) + 1e-8)\n","\n","  return filter_banks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBdjio78IIcD"},"outputs":[],"source":["def audioData_preprocessing(input_df, file_path, second=1, self=0):\n","  id_list = input_df.ID.tolist()\n","  data_size = input_df.shape[0]\n","  audio_feature = []\n","  for i in range(0, data_size):\n","      audio_feature.append(MFCCs(file_path + \"{}.wav\".format(id_list[i]), second))\n","\n","  return np.array(audio_feature)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bxqd9B8rpcn7"},"source":["## 訓練資料讀取"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":514,"status":"ok","timestamp":1684399722144,"user":{"displayName":"Han-Tang Chen","userId":"09799993280170327597"},"user_tz":-480},"id":"_UlEPSRBmszc","outputId":"0309a7b0-b360-4156-a7ba-2089041cf392"},"outputs":[{"name":"stdout","output_type":"stream","text":["(850, 26) (850, 101, 39) (850, 5)\n","(150, 26) (150, 101, 39) (150, 5)\n"]}],"source":["x_train_text = medical_data_proccessing(training_df).drop(['Disease category','ID'], axis=1).to_numpy()\n","x_val_text = medical_data_proccessing(validate_df).drop(['Disease category','ID'], axis=1).to_numpy()\n","\n","x_train_audio = audioData_preprocessing(training_df, file_path = \"./data/training_data/\")\n","x_val_audio = audioData_preprocessing(validate_df, file_path = \"./data/training_data/\")\n","\n","y_train = pd.get_dummies(training_df, columns=['Disease category']).to_numpy()[:,-5:].astype('float32')\n","y_val = pd.get_dummies(validate_df, columns=['Disease category']).to_numpy()[:,-5:].astype('float32')\n","\n","print(x_train_text.shape, x_train_audio.shape, y_train.shape)\n","print(x_val_text.shape, x_val_audio.shape, y_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdfuTtQ01K-M"},"outputs":[],"source":["x_train_multi = np.concatenate((x_train_text, x_train_audio.reshape(850,-1)), axis = 1)\n","x_val_multi = np.concatenate((x_val_text, x_val_audio.reshape(150,-1)), axis = 1)\n","\n","x_train_audio = audioData_preprocessing(training_df, file_path = \"./data/training_data/\", second = 3.5, self=1)\n","x_val_audio = audioData_preprocessing(validate_df, file_path = \"./data/training_data/\", second = 3.5, self=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XRKRUVCJox3K"},"source":["# Traditional Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Al7hoYKzxlYA"},"outputs":[],"source":["y_true = validate_df['Disease category'] - 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePjbcccmRwri"},"outputs":[],"source":["clf = LogisticRegression(class_weight=class_weight, max_iter=1000)\n","clf = clf.fit(x_train_text, np.argmax(y_train, axis=1))\n","y_pred = clf.predict(x_val_text)\n","\n","print(( clf.predict(x_train_text) == np.argmax(y_train, axis=1)).sum() / len(y_train))\n","print((clf.predict(x_val_text) == np.argmax(y_val, axis=1)).sum() / len(y_val))\n","\n","results_recall = recall_score(y_true, y_pred, average=None)\n","print(\"Test UAR(Unweighted Average Recall) :\", results_recall.mean())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GdjJNFrNJmUs"},"source":["# Intermediate Fusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlSdfil49OGX"},"outputs":[],"source":["audio_model = load_model(\"audio_model(0.53).h5\")\n","text_model = load_model(\"text_model.h5\")\n","multi_model = load_model(\"multi_model_600*50(0.33).h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":880,"status":"ok","timestamp":1684399818829,"user":{"displayName":"Han-Tang Chen","userId":"09799993280170327597"},"user_tz":-480},"id":"8Xioz7w84xwi","outputId":"fe8c7ec6-cc8d-46bf-cdb7-7074b2b5a90d"},"outputs":[{"name":"stdout","output_type":"stream","text":["5/5 [==============================] - 0s 46ms/step\n","5/5 [==============================] - 0s 4ms/step\n","0.6846882660836149\n","0.5308619890015238\n","0.33450606241303915\n"]}],"source":["y_true = validate_df['Disease category'] - 1\n","text_pred = np.eye(5)[clf.predict(x_val_text)]\n","audio_pred = np.eye(5)[audio_model.predict(x_val_audio).argmax(axis=1)]\n","multi_pred = np.eye(5)[multi_model.predict(x_val_multi).argmax(axis=1)]\n","\n","text_recall = recall_score(y_true, text_pred.argmax(axis=1), average=None).mean()\n","audio_recall = recall_score(y_true, audio_pred.argmax(axis=1), average=None).mean()\n","multi_recall = recall_score(y_true, multi_pred.argmax(axis=1), average=None).mean()\n","print(text_recall)\n","print(audio_recall)\n","print(multi_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684370472635,"user":{"displayName":"Han-Tang Chen","userId":"09799993280170327597"},"user_tz":-480},"id":"9BfmVZpGHOKl","outputId":"5ee94745-8a4d-4bfb-e1ba-4ec0feec553d"},"outputs":[{"name":"stdout","output_type":"stream","text":["(850, 3)\n"]}],"source":["def create_finalModel(input_shape, neurons = 20, hidden_layers = 3, learning_rate = 0.0001, verbose=0):\n","    model = Sequential()\n","\n","    model.add(Dense(neurons, input_dim=input_shape, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    #model.add(BatchNormalization())\n","    model.add(LayerNormalization())\n","\n","    for i in range(hidden_layers-1):\n","        model.add(Dense(neurons, activation='relu'))\n","        model.add(layers.Dropout(0.5))\n","        model.add(BatchNormalization())\n","        #model.add(LayerNormalization())\n","\n","    model.add(Dense(5, activation='softmax'))\n","\n","    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) # categorical_crossentropy\n","\n","    if verbose:\n","        model.summary()\n","    \n","    return model\n","\n","final_model = create_finalModel(input_shape = final_train_x.shape[1])\n","print(final_train_x.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2043,"status":"ok","timestamp":1684399442567,"user":{"displayName":"Han-Tang Chen","userId":"09799993280170327597"},"user_tz":-480},"id":"3q_Hlz41AopS","outputId":"fd8c6f14-23fb-471c-8082-1631457d2f11"},"outputs":[{"name":"stdout","output_type":"stream","text":["27/27 [==============================] - 1s 35ms/step\n","27/27 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 22ms/step\n","5/5 [==============================] - 0s 4ms/step\n"]}],"source":["\n","final_train_x = np.concatenate((clf.predict(x_train_text)[:,None],\n","                 audio_model.predict(x_train_audio).argmax(axis=1)[:,None], \n","                 multi_model.predict(x_train_multi).argmax(axis=1)[:,None]), axis = 1)\n","final_val_x = np.concatenate((clf.predict(x_val_text)[:,None], \n","                audio_model.predict(x_val_audio).argmax(axis=1)[:,None], \n","                multi_model.predict(x_val_multi).argmax(axis=1)[:,None]), axis = 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLwxpT5ITJP1"},"outputs":[],"source":["final_model.fit(final_train_x, y_train, batch_size=256, epochs=3000, class_weight = class_weight,\n","                callbacks=[ModelCheckpoint(\"final_model.h5\", save_best_only=True, monitor='val_accuracy')],\n","                validation_data=(final_val_x, y_val)\n","                )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glNzd80Eoqjb"},"outputs":[],"source":["# y_pred = final_model.predict(final_val_x).argmax(axis=1)\n","# y_pred = (text_model.predict(x_val_text) + audio_model.predict(x_val_audio) + multi_model.predict(x_val_muti)).argmax(axis=1)\n","y_pred = ((text_pred * text_recall + audio_pred * audio_recall + multi_pred * multi_recall)/(text_recall + audio_recall + multi_recall)).argmax(axis=1)\n","y_pred = (text_pred * 4.9 + audio_pred * 2.4 + multi_pred * 2.6).argmax(axis=1)\n","\n","results_recall = recall_score(y_true, y_pred, average=None)\n","print(\"Test UAR(Unweighted Average Recall) :\", results_recall.mean())\n","ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred)).plot(cmap='Blues')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Lv9UJi9nqYI5"},"source":["# Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RG8NXaMWKVi"},"outputs":[],"source":["test_df_pb = pd.read_csv('./data/Public Testing Dataset/test_datalist_public.csv')\n","x_test_audio_pb = audioData_preprocessing(test_df_pb, file_path = \"./data/Public Testing Dataset/test_data_public/\")\n","x_test_text_pb = medical_data_proccessing(test_df_pb).drop(['ID'], axis=1).to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kAuLjW6mTDAi"},"outputs":[],"source":["test_df_pv = pd.read_csv('./data/Private Testing Dataset/test_datalist_private.csv')\n","x_test_audio_pv = audioData_preprocessing(test_df_pv, file_path = \"./data/Private Testing Dataset/test_data_private/\")\n","x_test_text_pv = medical_data_proccessing(test_df_pv).drop(['ID'], axis=1).to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQxMN977T9f3"},"outputs":[],"source":["test_df = pd.concat([test_df_pb, test_df_pv])\n","x_test_audio = np.concatenate((x_test_audio_pb, x_test_audio_pv))\n","x_test_text = np.concatenate((x_test_text_pb, x_test_text_pv)) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KjUt-ZYZ90mV"},"outputs":[],"source":["x_test_multi = np.concatenate((x_test_text, x_test_audio.reshape(1000,-1)), axis = 1)\n","\n","x_test_audio_pv = audioData_preprocessing(test_df_pv, file_path = \"./data/Private Testing Dataset/test_data_private/\", second = 3.5, self=1)\n","x_test_audio_pb = audioData_preprocessing(test_df_pb, file_path = \"./data/Public Testing Dataset/test_data_public/\", second = 3.5, self=1)\n","x_test_audio = np.concatenate((x_test_audio_pb, x_test_audio_pv))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2076,"status":"ok","timestamp":1684399931506,"user":{"displayName":"Han-Tang Chen","userId":"09799993280170327597"},"user_tz":-480},"id":"uSm-3dk4HH62","outputId":"1af8d80b-07ef-4a74-8e28-bab77b59ec13"},"outputs":[{"name":"stdout","output_type":"stream","text":["32/32 [==============================] - 1s 27ms/step\n","32/32 [==============================] - 0s 2ms/step\n"]}],"source":["y_true = validate_df['Disease category'] - 1\n","text_pred = np.eye(5)[clf.predict(x_test_text)]\n","audio_pred = np.eye(5)[audio_model.predict(x_test_audio).argmax(axis=1)]\n","multi_pred = np.eye(5)[multi_model.predict(x_test_multi).argmax(axis=1)]\n","\n","# prediction of tradition model\n","y_pred = (text_pred * text_recall + audio_pred * 0 + multi_pred * 0).argmax(axis=1)\n","\n","my_submission = pd.DataFrame({'Id': test_df.ID, 'category': y_pred + 1})\n","# you could use any filename. We choose submission here\n","my_submission.to_csv('submission8.csv', index=False, header=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"modelenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"f97432c16914304dbd818b138841742b9483a5a148ca981647dc7438178b3282"}}},"nbformat":4,"nbformat_minor":0}
